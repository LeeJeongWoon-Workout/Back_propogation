class Neuron:
    def __init__(self, W, b, a_obj):
        # Model parameters
        self.W = W
        self.b = b
        self.a = a_obj()
        
        # gradient
        self.dW = np.zeros_like(self.W)
        self.db = np.zeros_like(self.b)
        self.dh = np.zeros_like(_t(self.W))
        
        self.last_x = np.zeros((self.W.shape[0]))  #이전 값을 저장해야 다시 불러올 수 있다. w에 대해서 미분 할 때
        self.last_h = np.zeros((self.W.shape[1])) #  1회 선형 결합을 나타낼때 활성함수에 넣기전을 표현했음

    def __call__(self, x):
        self.last_x = x
        self.last_h = _m(_t(self.W), x) + self.b
        return self.a(self.last_h)

    def grad(self): # dy/dh = W
        return self.W * self.a.grad()

    def grad_W(self, dh):
        grad = np.ones_like(self.W)
        grad_a = self.a.grad()
        for j in range(grad.shape[1]): # dy/dw = x
            grad[:, j] = dh[j] * grad_a[j] * self.last_x  #last에 dh는 지금까지의 누적
        return grad

    def grad_b(self, dh): # dy/dh = 1
        return dh * self.a.grad() #지금까지 넘어온 gradient의 누적을 dh 로 정의한다.
